
\chapter{Introduction}
%\ac{talk about GGP}
General Game Playing (GGP) is a framework in which artificial intelligence programs are required to play a large number of games successfully.\cite{Genesereth/GGPOverview}.
Games in the framework range in both number of players and complexity; from the single player Eight Puzzle to the six player Chinese Checkers, and from the relativly simple Rock Paper Scissors to Chess\cite{GGP-Website}. Every year the GGP competition takes place, aiming to find the best GGP program.
\ac{Why is GGP interesting? You need to motivate the work. You want to make a reader excited to read on.}.
A consequence of this framework is large repositories of game descriptions written in the Game Description Language (GDL), a logic programming language built for describing games as state machines\cite{GDL_Spec}. A logic programming language being any that is mainly based on formal logic, such as Prolog.

\ac{give an example of a GGP game}

%\ac{talk about IGGP}

These GDL game descriptions form the basis for the Inductive General Game Playing (IGGP) problem. The task is an inversion of the GGP problem. Rather than taking game rules using them to play the game in IGGP the aim is to learn the rules from observations of gameplay. Cropper et al. define the IGGP problem in their 2019 paper; given a set of gameplay observations the goal is to induce the rules of the game\cite{Cropper/IGGP}. The games are those from the GGP competition problem set, specified in GDL, meaning they are widely varied in complexity.
%\ac{do not mention my name as there are several authors. Instead say Cropper et al. define ...}

\ac{give an example of a IGGP problem}

%\af{GIVE EXAMPLE OF IGGP PROBLEM}

A way of solving this problem is Inductive Logic Programming (ILP).
%\ac{In their empirical evaluations, the authors showed that ILP systems achieve the best score ... }
In ILP, machine learning systems are tasked with learning logic programs given some background knowledge and a set of values for which the programs are true or false. The ILP system will derive a hypothesis, a logic program that when combined with the background knowledge, entails all of the positive and none of the negative examples\cite{Muggleton/ILP}. In the IGGP paper it is shown that the problem is hard for current ILP systems, with on average only 40\% of the rules being learned by the best performing systems.

%\ac{However, a limitation of existing work is ....}

\ac{It is best to start a paragraph with the key statement. I suggest you literally start with ``However, a limitation of existing work is ....''}.
In this paper we try and increase the success rate of the ILP systems.
\ac{That is not the goal of the work. The goal of the work is to investigate whether varying the quality of game traces influences the ability for ILP systems to solve IGGP problems.}
We use the IGGP framework to evaluate the ability of ILP agents to correctly induce the rules of a game given different sets of gameplay observations - optimal gameplay verses random gameplay as well as combinations of the two. So far no work has been done on how the observations given as training data to the systems affect their ability to learn games.


It is not obvious whether random or optimal gameplay would be best. When learning the rules of chess would a human rather watch moves being made randomly, or a match between two grandmasters? It is not an easy question to answer. Both situations will result in a restricted view of the game, with curtain situations never occurring in each one. It is possible watching both types of play viewed together would give a better understanding which is why we test on a mixture of optimal and random traces together.


\ac{
I really like this paragraph and your example.
I suggest expanding on it a little bit.
You could try to make the statement broader by saying that the problem of the quality of examples is not limited to just game playing.
For instance, if we are teaching cars to drive, we could clearly not train them on optimal formula 1 quality driving.
Likewise, we would not train them on random driving.
Where does the sweetspot lie?
In this paper, we try to help give some insight into such a fundamental question.
Specifically, we ask the following research questions:
}

%\ac{Make the research questions clear: here are three that I have just brain dumped}

\begin{description}
\item[Q1] Does varying the quality of game traces influence the ability for learners to solve the IGGP problems? Specifically, does the quality of game place affect predictive accuracy?
\item[Q2] Does varying the amount of game traces influence the ability for learners to solve the IGGP problems? Specifically, does the quality of game place affect predictive accuracy?
\item[Q3] Can we improve the performance of a learner by mixing the quality of traces?
\end{description}

\ac{you should comment on these as to link to the next paragraph, e.g. why are you asking them? Are the answers obvious? If not, why (the why is the following paragraph)}

%\ac{Why is it interesting?}
We would also expect models trained on the same distribution as they are tested on to perform best since it is generally accepted that the accuracy of a model increases the closer the test data distribution is to the training data distribution.\cite{Mitchell/MachineLearing}. However, Gonzales \cite{Gonzalez/MismatchedOutperform} suggests that a system trained on a different domain to the one it is tested can outperform a system trained and tested on the same domain. In his 2010 paper Ben-David shows that training data taken from multiple different domains can in fact give lower error on testing data that traning data taken from any single domain, including the testing domain \cite{Ben-David/DifferentDomains}. It is not clear in our case what selection of training data will result in the most effect learning.
\ac{this paragraph is good and is very interesting. I suggest slightly expanding on each reference to give the reader more intuition about those works}.

In machine learning the probably approximately correct learning framework (PAC) is a mathematical model that can be applied to machine learning systems for analysis. It gives an upper bound on the number of training examples needed for a learner to probably (with probability at least $[1-\delta]$) learn a hypothesis that is approximately (within error $\epsilon$)correct\cite{Mitchell/MachineLearing}. In section ? we test to find the number of examples beyond which the reduction in $\epsilon$ and $\delta$ becomes negligible.
\ac{you can cut this paragraph}

\ac{To help answer questions 1-3, we make the following contributions:}

\ac{This part is vital. Make it clear and precise what you have done. As I said before, this bullet pointed list is one of the key parts of your report}
\subsubsection{Contributions}
\begin{itemize}
\item \ac{We implement a system to play GGP games at (1) random, (2) world-leading, and (3) optimal levels. (Section \ref{somesection})}
\item We transform the GGP traces to IGGP problems
\item We train the ILP systems \ac{what ILP systems}?on different combinations of optimal and random data as well as testing them on each individually (Section \ref{somesection})

\item We experimentally explore the research questions (Q1, Q2, and Q3). Our results show ...
\end{itemize}