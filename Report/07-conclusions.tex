\chapter{Conclusions}
In this paper we have compared the ability of ILP systems to learn the rules of a game from gameplay observations in the IGGP framework. Three ILP systems have been trained on a range of different datasets. These datasets consisted of random  gameplay, intelligent gameplay as generated by the Sancho system, and a mix of both. The generated hypotheses of the learners have been tested on random and intelligent gameplay from the same distributions as the training data. Differences in the effectiveness of the learned programs in correctly classifying legal and illegal gameplay were observed however there was not enough statistical significance in the results to disprove the null hypothesis.

To test the significance of the size of the training dataset on the ability of the ILP systems to learn the systems were trained on varying numbers of game traces. The results of the experiment were inconclusive as not all the systems had enough time to learn a hypothesis when trained on the large datasets.


\subsubsection{Limitations}
There are several aspects that could be improved on in this paper.
\\

\textbf{Computational resources}
The computational resources and thus the time given for each system to learn the rules were severely limited for the experiments in this paper. If they were to be conducted again with greater computational resource with more time given to each system more significant results may be achieved.

\textbf{ILP systems} In this paper we only test on three ILP systems, it is clear a more representative result could be obtained by testing on more systems. Techniques such as probabilistic \cite{Bellodi/Probablistic,Raedt/Probabalistic} and interactive \cite{Raedt/Interactive} learning are not used by any of the systems tested here. ILASP is the only system designed to handle noisy data \cite{MarkLaw/ILASP2i}, it would be interesting to try others with this approach \cite{Oblak/Noise,Evans/Noise}. The systems used also have many customisable settings for example Metagols metarules which have major implications for the programs learnt \cite{Cropper/Metarules}. Aleph has many different search methods on the hypothesis space many of which may yield better results than the default algorithm used in this paper.

\textbf{Generating traces} Whilst Sancho generates good examples of intelligent play it would be interesting to see traces generated by other general game playing methods \cite{Park/GGPAdvances,Kowalski/GGP}. For some games there exists a provably optimal set of moves in some cases such as for \textit{eight puzzle} Sancho generates these moves (see section \ref{sec:sancho}) however it does not for all such games \cite{Schaeffer/Checkers}. Future research could compare the effects on the learned hypotheses when trained on optimal data. It may also be more insightful to look at the difference between human generated data and random or optimal.

\textbf{Other machine learning systems} There exists a huge variety of machine learning systems other than ILP. It is possible some of these may exhibit more of a bias toward one of the training sets. Testing a neural network or genetic algorithm based approach may provide insightful results.

\textbf{Intelligent versus random in other contexts} There are plenty of other domains other than game playing in which datasets can be considered intelligent or random. An example of this might be training a car to drive where you could train it on different quality levels of driving or training a facial recognition system on poor or high quality photos. These would prove an interesting area to study.

\textbf{Levels of intelligence} So far we have only considered a single level of intelligent play. It is clear that in humans for any given game we do not simply have good players and random players. There is a scale of skill. The ELO system captures this concept well and is used as a generic measure of skill for zero sum games but has been extended to more \cite{ELO}. It would be informative to pick a game and use game traces generated by player of a range different ELO score. It would then be clear if the systems in fact learned best from a particular level of play rather than just optimal or random. We expect that the best level of play for training would differ among games as has been shown in this paper. However it would still be useful to discover what level was optimal for any game. 

\textbf{How do humans compare} We have spent a lot of this paper investigating whether machines learn better on intelligent gameplay or random gameplay with the assumption that a human would learn better from watching another human play than random legal moves. Whilst in reality we might generally choose to watch a human playing when trying to learn a game rather than observing random legal moves it may not be the most effective way. Any real world example of this is almost always accompanied by additional information. For example humans would often have access to some description of the rules or someone explaining the edge cases that may not occur. It is hard to compare how humans learn to how machines learn since the background knowledge of a human is so much greater. It would be interesting to see how humans would fare in similar experiments to those in this paper. Looking into this question would give new insights into the results of this paper. As with the similar question posed in this paper the experiment does not have clear predicted outcome.


