\begin{abstract}

General Game Playing (GGP) is a framework in which an artificial intelligence program is required to play a variety of games successfully, it acts as a test bed for AI and motivator of research. The AI is given a random game description at runtime (such as checkers or tic tac toe) which it then plays.
The framework includes repositories of these game descriptions written in a logic programming language.


The Inductive General Game Playing (IGGP) problem challenges machine learning systems to learn these GGP game rules by watching the game being played.
%\ac{The above statement is precise. You could use a different ML approach (as we do in the IGGP paper).}
In other words, IGGP is the problem of inducing general game rules from specific game observations. Inductive Logic Programming (ILP), a subsection of ML has proved to be a promising approach to this problem though it has been shown that it is still a hard problem for ILP systems. In this regard IGGP motivates future research in ILP.

Existing work on IGGP has always assumed that the game player being observed makes random moves. This is not representative of how a human learns to play a game, to learn to play chess we watch someone who is playing to win. 
\ac{emphasise this limitation more, why is your work interesting? why should anyone care?}

To address this limitation, in this paper we analyse the effect of using optimal verses random gameplay traces as well as the effect of varying the number of traces in the training set.

We use Sancho, the 2014 GGP competition winner, to generate optimal game traces for a large number of games.
We then use the ILP systems, Metagol, Aleph and ILASP to induce game rules from the traces.
\ac{are trained and tested with combinations of optimal and random data. <- be very clear what you do}
%\ac{Write with an active voice 'We use Sacho ...'}

\ac{Our results show [can fill in later]}

\ac{The implications of this work are [can fill in later]}



% When learning programs in Inductive Logic Programming (ILP) optimisations to the dataset used to train the systems can often be as effective as improvements to the systems themselves.


\end{abstract}