% \chapter{Logical setting}
\chapter{IGGP problem}
\label{ch:IGGP}

The IGGP problem is defined in the 2019 paper Inductive General Game Playing \cite{Cropper/IGGP}. Much like the problem of ILP described in section \ref{sec:ILP} the problem setting consists of examples about the truth or falsity of a formula $F$ and a hypothesis $H$ which covers $F$ if $H$ entails $F$. We assume the languages of:
\begin{itemize}
\item $\mathscr{E}$ the language of examples (observations)
\item $\mathscr{B}$ the language of background knowledge
\item $\mathscr{H}$ the language of hypotheses
\end{itemize} 
Each of these languages can be see as a subset of those defined for the ILP task. These languages are made up only of function free ground atoms\cite{Uwe/Logic}. In our experiments we transform data from the GDL descriptions of games in the IGGP dataset and transform it into the languages of $\mathscr{B}$ and $\mathscr{E}$. GDL allows for functions (predicates nested in one another) in rules albeit in a restricted form. For example any atom appearing inside a \texttt{true} predicate such as \texttt{true(count(9))}. We flatten all of these to single, non nested predicates, i.e. \verb|true_count(9)|. This is needed as not all ILP systems support function symbols. We can therefore assume that both $\mathscr{E}$ and $\mathscr{B}$ are function-free. The language of hypotheses $\mathscr{H}$ can be assumed to consist of datalog programs with stratified negation as described here\cite{Kenneth}. Stratified negation is not necessary but in practice allows significantly more concise programs, and thus often makes the learning task computationally easier. We first define an IGGP \textit{input} then use it to define the IGGP \textit{problem}. The IGGP input needs to capture the idea of a set of observations about a single game. The input is based on the general input for the Logical induction problem.

\textbf{The IGGP Input:} An input $\Delta$ is a set of triples $\{(B_i,E_i^+,E_i^-)\}_m^{i=0}$ where
\begin{itemize}
\item $B_i \subset \mathscr{B}$ represents background knowledge
\item $E_i^+ \subseteq \mathscr{E}$ and $E_i^- \subseteq \mathscr{E}$ represent positive and negative examples respectively
\end{itemize}
The IGGP input composes the IGGP problem as follows:

\textbf{The IGGP Problem:} Given an IGGP input $\Delta$, the IGGP problem is to return a hypothesis $H \in \mathscr{H}$ such that for all $(B_i,E_i^+,E_i^-) \in \Delta$ it holds that $H \cup B_i \models E_i^+$ and
$H \cup B_i \not\models E_i^-$.

In this paper we use the IGGP problem to analyse ability of ILP agents to learn from on a range of training data of differing quality. To analyse the ability of the ILP systems to learn we use, for each IGGP task $\Delta$ and hypothesis $H$, a testing set $T = T^+\cup T^-$ where
\begin{itemize}
	\item $T^+ \subseteq \mathscr{E}$ and $T^-\subseteq \mathscr{E}$ are the positive and negative testing observations.
	\item $T^+ \cap \bigcup_{i=0}^m E_i^+  = \emptyset$. The positive testing examples are distinct from the positive training examples
	\item $T^- \cap \bigcup_{i=0}^m E_i^-  = \emptyset$. The negative testing examples are distinct from the negative training examples
\end{itemize}
To test the systems we check for each $t^+ \in T^+$ weather\[ H \cup \bigcup_{i=0}^m B_i \models t^+\] and for each $t^- \in T^-$  weather \[ H \cup \bigcup_{i=0}^m B_i \not\models t^-\]
The success of the system is defined as the percentage of correctly classified $t \in T$. We use this definition of success to answer the research questions in chapter \ref{ch:intro}. The experiments preformed to answer these questions are described in chapter \ref{ch:methodology}.

